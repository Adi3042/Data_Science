{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fde095-e1c0-421b-971e-ffdf2b459d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique that scales numerical features to \n",
    "a specific range, typically between 0 and 1. It is used to ensure that all features have the same scale, preventing \n",
    "some features from dominating others due to their magnitude.\n",
    "\n",
    "Example:\n",
    "    Suppose you have a feature \"age\" with values ranging from 0 to 100. \n",
    "    After Min-Max scaling, \n",
    "        these values will be transformed to the range [0, 1], preserving their relative proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffaea3b-05b6-46b2-85b4-d5c5c0b3bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "The unit vector technique, also known as vector normalization, scales features to have a length of 1 while preserving \n",
    "their direction. It is often used in machine learning algorithms that rely on the magnitude of vectors, like cosine similarity.\n",
    "\n",
    "Example:-\n",
    "    If you have a vector [3, 4], its unit vector would be [0.6, 0.8]. \n",
    "        The values are scaled, but the direction remains the same.\n",
    "\n",
    "Differences:-\n",
    "    Min-Max scaling scales features to a specific range, usually [0, 1], while the unit vector technique scales to unit length.\n",
    "    Min-Max scaling is used to scale individual features, while the unit vector technique is used for scaling vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978307e8-0265-4a16-b5db-bf8f70a3a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "\n",
    "PCA is a dimensionality reduction technique that identifies the most important linear combinations of features in a dataset. \n",
    "It is used to reduce the number of features while preserving as much variance as possible.\n",
    "\n",
    "Example: \n",
    "    In a dataset with many correlated features (e.g., height, weight, age), PCA can create new features that capture the most \n",
    "    significant variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da316bd-6ebd-478d-81e6-a7e2f799aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "\n",
    "PCA is a feature extraction technique that transforms the original features into a set of orthogonal principal components. \n",
    "These components can be used as new features that often capture the most important information in the data.\n",
    "\n",
    "Example: \n",
    "    In a dataset with features like height, weight, and age, PCA can be applied to extract principal components. \n",
    "    These components can represent combinations of the original features (e.g., \"body size\" component)\n",
    "    and be used for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9be5c0-e651-425d-acba-2a31a240d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "\n",
    "For each feature:-\n",
    "\n",
    "    (i). Calculate the minimum (min_val) and maximum (max_val) values within the dataset for that feature.\n",
    "\n",
    "    (ii). For each data point, apply the Min-Max scaling formula:-\n",
    "                Scaled_Value = (Value - min_val) / (max_val - min_val)\n",
    "\n",
    "    (iii). Repeat this process for all features, ensuring that they are all scaled to the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a758e3-ca92-4a60-9a97-6d34a5becb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "\n",
    "To reduce the dimensionality of the dataset for stock price prediction, you can use PCA as follows:-\n",
    "\n",
    "(i). Standardize the features:-\n",
    "    Ensure that all features have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "(ii). Calculate the covariance matrix:-\n",
    "    Compute the covariance matrix of the standardized features.\n",
    "\n",
    "(iii). Perform PCA:-\n",
    "    Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "(iv). Select the number of components:-\n",
    "    Decide how many principal components to retain based on the cumulative explained variance. \n",
    "\n",
    "(v). Transform the data:-\n",
    "    Project the original data onto the selected principal components to create a new dataset with reduced dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a1909-18e3-4d7a-9d04-9629a0862c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ed4f1f-c19a-4962-9af6-3ba0edace951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-1.00', '-0.58', '-0.05', '0.47', '1.00']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "min_range = -1\n",
    "max_range = 1\n",
    "\n",
    "min_val = data.min()\n",
    "max_val = data.max()\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = (data - min_val) / (max_val - min_val) * (max_range - min_range) + min_range\n",
    "\n",
    "print([\"{:.2f}\".format(i) for i in scaled_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bd5a1-7d73-42aa-9bd1-affb2d07eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed878a2e-87cd-4ef9-9bf0-d933f3ec1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'height': [165, 175, 160, 180, 170],\n",
    "    'weight': [60, 70, 55, 75, 65],\n",
    "    'age': [30, 25, 35, 40, 28],\n",
    "    'gender': ['Male', 'Male', 'Female', 'Male', 'Female'],\n",
    "    'blood_pressure': ['120/80', '130/85', '115/75', '140/90', '125/78']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "gender_encoded = encoder.fit_transform(df[['gender']]).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(df[['height', 'weight', 'age']])\n",
    "\n",
    "X = pd.DataFrame(data=numerical_features, columns=['height', 'weight', 'age'])\n",
    "X['gender_Male'] = gender_encoded[:, 0]\n",
    "X['gender_Female'] = gender_encoded[:, 1]\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "# Determine the number of components to retain \n",
    "n_components = (cumulative_explained_variance < 0.95).sum() + 1  # Add 1 for the first component\n",
    "print(f\"Number of components to retain: {n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e6ea4-9505-4a88-9d53-418beab07053",
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of principal components to retain in PCA depends on your specific goals and the variance explained\n",
    "by each principal component. Generally, you want to retain enough principal components to capture a high\n",
    "percentage of the total variance in the data while reducing dimensionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
