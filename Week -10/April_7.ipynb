{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bf4628-ae93-49da-add7-19789f776b62",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\" style=\"padding: 10px;\">    \n",
    "    <h1><b><u>Support Vector Machines-2</u></b></h1>\n",
    "</div>\n",
    "\n",
    "**Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?**\n",
    "\n",
    "\n",
    "In short, polynomial kernel functions are a type of kernel function used in machine learning algorithms like SVMs.\n",
    "\n",
    "They enable SVMs to find non-linear decision boundaries by transforming data into a higher-dimensional space using polynomial\n",
    "functions, making it easier to separate classes that are not linearly separable in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d957a-a928-45d8-bd09-9b955f3d4747",
   "metadata": {},
   "source": [
    "---\n",
    "**Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?**\n",
    "\n",
    "\n",
    "**To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can follow these steps:-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b2c77f-164f-4e65-9b67-54a14a983fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Instance of the SVC classifier with a polynomial kernel\n",
    "svc_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_poly.fit(X_train, y_train)\n",
    "y_pred = svc_poly.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf76abd-7f10-4c56-b899-2786c580dd1b",
   "metadata": {},
   "source": [
    "---\n",
    "**Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?**\n",
    "\n",
    "Increasing the value of epsilon affects the number of support vectors in SVR as follows:-\n",
    "\n",
    "- Increasing epsilon in SVR results in a larger margin of error tolerance.\n",
    "- A larger epsilon means more data points can be within the margin and become support vectors.\n",
    "- This leads to a smoother and less complex regression model.\n",
    "- The choice of epsilon should be based on the problem's needs and the trade-off between complexity and generalization.\n",
    "- Larger epsilon values can yield simpler models with potentially higher bias and lower variance.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?**\n",
    "\n",
    "**(i). Kernel function:-**\n",
    "\n",
    "- **Choice:-** The kernel function determines the type of transformation applied to the data. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "- **Influence:-** The choice of the kernel function affects how the SVR model captures non-linear relationships in the data. Select a kernel function based on the underlying problem's characteristics. For complex non-linear relationships, RBF or polynomial kernels may be suitable.\n",
    "\n",
    "**(ii). C parameter:-**\n",
    "\n",
    "- **Purpose:-** C is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the training error. Smaller C values result in larger margins but may allow more training errors, while larger C values prioritize reducing errors but may lead to overfitting.\n",
    "- **Increase:-** Increase C when you suspect the model is underfitting or the training error needs to be reduced.\n",
    "- **Decrease:-** Decrease C when you want to encourage a wider margin and prevent overfitting.\n",
    "\n",
    "**(iii). Epsilon parameter (ε):-**\n",
    "\n",
    "- **Purpose:-** Epsilon determines the width of the tube around the regression line within which data points are considered correctly predicted. It controls the margin of tolerance for errors.\n",
    "- **Increase:-** Increase ε if you want to allow more data points to fall within the margin, resulting in a smoother model.\n",
    "- **Decrease:-** Decrease ε to make the model more sensitive to errors and have a narrower margin.\n",
    "\n",
    "**(iv). Gamma parameter (only for RBF kernel):-**\n",
    "\n",
    "- **Purpose:-** Gamma determines the shape of the RBF kernel and the influence of each training example. Smaller values make the influence of training examples more widespread, while larger values make it more localized.\n",
    "- **Increase:-** Increase gamma when you suspect the relationship between inputs and outputs is highly localized and you want the model to focus on nearby data points.\n",
    "- **Decrease:-** Decrease gamma when you want the model to consider a broader range of data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce9014-5f15-4a2d-9993-a82f66e2450a",
   "metadata": {},
   "source": [
    "---\n",
    "**Q5. Assignment:**\n",
    "```python\n",
    "        Import the necessary libraries and load the dataset\n",
    "        Split the dataset into training and testing setZ\n",
    "        Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "        Create an instance of the SVC classifier and train it on the training datW\n",
    "        hse the trained classifier to predict the labels of the testing datW\n",
    "        Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "        precision, recall, F1-scoreK\n",
    "        Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "        improve its performanc_\n",
    "        Train the tuned classifier on the entire dataseg\n",
    "        Save the trained classifier to a file for future use.\n",
    "```\n",
    "\n",
    "**You can use any dataset of your choice for this assignment, but make sure it is suitable for classification and has a sufficient number of features and samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f363e4-b3ee-427e-8e92-d17c5e9e5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f69971-4c83-46a4-9d96-8d23022cb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "with open('tuned_svc_classifier.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_svc, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
