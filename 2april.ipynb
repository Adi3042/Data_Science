{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857fc0a7-7fa5-4ee1-b9c0-01765d205aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "\n",
    "    Grid search CV is a hyperparameter tuning technique that systematically evaluates a combination of hyperparameters\n",
    "    for a machine learning model. It works by creating a grid of all possible hyperparameter values and then training\n",
    "    and evaluating the model on each combination of hyperparameters. The best model is then selected based on a \n",
    "    performance metric such as accuracy or F1-score.\n",
    "\n",
    "    Grid search CV is a powerful hyperparameter tuning technique, but it can be computationally expensive, especially\n",
    "    for models with a large number of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b8911-1d4f-4cdd-8a59-7a74ba55d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "    one over the other?\n",
    "\n",
    "\n",
    "    Grid search CV works by exhaustively evaluating all possible combinations of hyperparameter values. \n",
    "    This can be computationally expensive, especially for models with a large number of hyperparameters. \n",
    "    However, grid search CV is guaranteed to find the best possible hyperparameter combination, if it exists.\n",
    "\n",
    "    Randomized search CV works by randomly sampling a subset of hyperparameter combinations to evaluate. \n",
    "    This is less computationally expensive than grid search CV, but it is also less likely to find the \n",
    "    best possible hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb32763-95f1-4a3d-b7a0-6677f6532c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "\n",
    "    Data leakage is a problem in machine learning that occurs when the training data contains information\n",
    "    about the target variable that is not available at prediction time. This can lead to the model \n",
    "    overfitting the training data and performing poorly on new data.\n",
    "\n",
    "    Example:-\n",
    "    \n",
    "        Lets say you are building a credit risk model, and the dataset contains a feature indicating the current \n",
    "        outstanding balance of a customers loan. If this feature is included in the training data and used for\n",
    "        model training, it may lead the model to learn patterns that would not be available in practice.\n",
    "        This can result in an overfit model that performs poorly on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d150c-dd68-4618-b4fa-25ed36a7f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "\n",
    "    To prevent data leakage in machine learning, make sure your training data does not contain information that\n",
    "    is not available at prediction time.\n",
    "\n",
    "    This means:-\n",
    "\n",
    "        (i). Removing any variables that are only available at training time.\n",
    "        (ii). Using cross-validation to train and evaluate your model on different subsets of the training data.\n",
    "        (iii). Only using features that are relevant to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97130d-6d97-4261-8b91-78da4f08fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "\n",
    "    A confusion matrix is a table used in the evaluation of classification models. \n",
    "    It provides a comprehensive view of the models performance by breaking down the predictions into four categories:-\n",
    "\n",
    "        (i). True Positives (TP):-\n",
    "            \n",
    "                The number of correctly predicted positive instances.\n",
    "            \n",
    "        (ii). True Negatives (TN):-\n",
    "            \n",
    "                The number of correctly predicted negative instances.\n",
    "            \n",
    "        (iii). False Positives (FP):-\n",
    "            \n",
    "                The number of negative instances incorrectly classified as positive (Type I error).\n",
    "            \n",
    "        (iv). False Negatives (FN):-\n",
    "            \n",
    "                The number of positive instances incorrectly classified as negative (Type II error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcfe50-be79-498d-9d65-9618191efc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "\n",
    "    Precision:-\n",
    "        Precision measures the accuracy of positive predictions made by the model. \n",
    "\n",
    "        It is calculated as:-\n",
    "\n",
    "            TP / (TP + FP) \n",
    "\n",
    "        and represents the proportion of positive predictions that were correct.\n",
    "\n",
    "    Recall:-\n",
    "\n",
    "        Recall, also known as sensitivity or true positive rate, measures the models ability to correctly \n",
    "        identify all relevant instances. \n",
    "\n",
    "        It is calculated as:-\n",
    "\n",
    "            TP / (TP + FN) \n",
    "\n",
    "        and represents the proportion of actual positive instances that were correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea54d0d-eee0-41db-9a4f-22e5ab4befc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "\n",
    "    To interpret a confusion matrix and identify the types of errors your model is making:-\n",
    "\n",
    "    a). High False Positives (FP):-\n",
    "    \n",
    "            (i). Your model is incorrectly classifying negative instances as positive. \n",
    "        \n",
    "            (ii). This may indicate that your model has a high false alarm rate.\n",
    "\n",
    "    b). High False Negatives (FN):-\n",
    "    \n",
    "            (i). Your model is incorrectly classifying positive instances as negative.\n",
    "        \n",
    "            (ii). This may indicate that your model is missing important positive cases.\n",
    "\n",
    "    c). High True Positives (TP) and True Negatives (TN):-\n",
    "    \n",
    "            (i). Your model is correctly classifying both positive and negative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95feb7-891b-428c-83fa-13ef76393e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "\n",
    "\n",
    "Common metrics derived from a confusion matrix include:-\n",
    "\n",
    "Accuracy:-\n",
    "    \n",
    "    Accuracy measures the overall correctness of predictions and is calculated as:-\n",
    "        (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "F1-Score:-\n",
    "\n",
    "    The F1-Score is the harmonic mean of precision and recall, giving a balance between the\n",
    "    two metrics. \n",
    "    \n",
    "    It is calculated as:- \n",
    "    \n",
    "        2 * (Precision * Recall) / (Precision + Recall)\n",
    "        \n",
    "\n",
    "Specificity (True Negative Rate):-\n",
    "\n",
    "    Specificity measures the models ability to correctly identify negative instances and \n",
    "    it is calculated as:- \n",
    "    \n",
    "        TN / (TN + FP).\n",
    "\n",
    "False Positive Rate:-\n",
    "\n",
    "    The false positive rate measures the proportion of actual negatives incorrectly classified as \n",
    "    positives and is calculated as:- \n",
    "\n",
    "        FP / (TN + FP)\n",
    "\n",
    "    \n",
    "ROC-AUC: \n",
    "    \n",
    "    Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) measures the models ability to distinguish \n",
    "    between positive and negative instances by plotting the ROC curve and calculating the area under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb6930-cf8b-42b3-ad4e-d6c92b204c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "\n",
    "    The accuracy of a model is a measure of its overall correctness and is directly related to the values\n",
    "    in its confusion matrix. \n",
    "    \n",
    "    Specifically, \n",
    "    \n",
    "    (i). Accuracy is calculated as (TP + TN) / (TP + TN + FP + FN). \n",
    "    (ii). It represents the proportion of all predictions that were correct.\n",
    "\n",
    "    However, accuracy alone may not provide a complete picture of model performance, especially when dealing with imbalanced datasets.\n",
    "    In such cases, its essential to consider other metrics like precision, recall, F1-Score, and ROC-AUC, which take into account\n",
    "    the distribution of true positive, true negative, false positive, and false negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bc9a5-2c49-417d-a74a-31ec1b7d9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "    model?\n",
    "\n",
    "\n",
    "    Check Class Distribution:-\n",
    "\n",
    "        (i). Look at the actual class distribution in your dataset for imbalances.\n",
    "        (ii). Check if the model disproportionately predicts one class over others.\n",
    "    \n",
    "    \n",
    "    Compare Class Metrics:-\n",
    "\n",
    "        (i). Calculate metrics (precision, recall, etc.) for each class using the confusion matrix.\n",
    "        (ii). Compare these metrics across classes; significant differences may indicate bias.\n",
    "    \n",
    "    \n",
    "    Analyze Misclassifications:-\n",
    "\n",
    "        (i). Examine the confusion matrix for patterns in misclassifications.\n",
    "\n",
    "        (ii). Identify classes frequently misclassified as others.\n",
    "    \n",
    "    Consider Demographic or Feature Biases:-\n",
    "\n",
    "        (i). Assess whether specific demographics or features lead to higher error rates.\n",
    "\n",
    "        (ii). Examine subgroup performance for fairness concerns.\n",
    "    \n",
    "        (iii). Utilize Fairness Metrics:\n",
    "\n",
    "            Apply fairness metrics like disparate impact or demographic parity to quantify bias formally.\n",
    "    \n",
    "    Iterate and Improve:\n",
    "\n",
    "        (i). Address identified biases by adjusting data, model, or features.\n",
    "        \n",
    "        (ii). Continuously monitor and improve fairness as part of model development.\n",
    "        \n",
    "        (iii). Identifying and mitigating biases is crucial for ethical and reliable machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
