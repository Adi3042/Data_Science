{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5310b901-803a-41b9-959f-69d799998d40",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\" style=\"padding: 10px;\">    \n",
    "    <h1><b><u>Exploratory Data Analysis-1</u></b></h1>\n",
    "</div>\n",
    "\n",
    "**Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.**\n",
    "\n",
    "Difference between Simple Linear Regression and Multiple Linear Regression:-\n",
    "\n",
    "- **Simple Linear Regression:**\n",
    "  - Involves modeling the relationship between a single independent variable and a dependent variable using a linear equation.\n",
    "  - Assumes a linear relationship between the predictor and the target.\n",
    "  - *Example:* Predicting a student's test score based on the number of hours they studied.\n",
    "\n",
    "- **Multiple Linear Regression:**\n",
    "  - Extends simple linear regression to model the relationship between multiple independent variables and a dependent variable.\n",
    "  - Assumes a linear relationship between all predictors and the target.\n",
    "  - *Example:* Predicting a house's price based on square footage, number of bedrooms, and neighborhood.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?**\n",
    "\n",
    "Assumptions of Linear Regression:-\n",
    "\n",
    "- **Linearity:** The relationship between predictors and the target is linear.\n",
    "- **Independence:** Residuals are independent of each other.\n",
    "- **Normality:** Residuals follow a normal distribution.\n",
    "- **Multicollinearity:** Predictors are not highly correlated.\n",
    "\n",
    "Check these assumptions by:-\n",
    "\n",
    "- Visual inspection (scatter plots, residual plots, Q-Q plots).\n",
    "- Statistical tests (Shapiro-Wilk test for normality, Durbin-Watson test for independence).\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.**\n",
    "\n",
    "Interpretation of Slope and Intercept in Linear Regression:-\n",
    "\n",
    "- **Slope:**\n",
    "  - Represents the change in the target variable for a one-unit change in the predictor variable, holding other predictors constant.\n",
    "  - *Example:* A slope of 2 for hours studied means that for every additional hour studied, the expected test score increases by 2 points.\n",
    "\n",
    "- **Intercept:**\n",
    "  - Represents the predicted value of the target variable when all predictor variables are set to zero.\n",
    "  - May lack a meaningful interpretation in some cases but is part of the linear equation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. Explain the concept of gradient descent. How is it used in machine learning?**\n",
    "\n",
    "Gradient descent is an optimization algorithm used in machine learning to minimize the loss or cost function of a model. It involves iteratively adjusting model parameters to find values that minimize the loss function. It is widely used in training various machine learning models, including linear regression and neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?**\n",
    "\n",
    "Multiple linear regression models the relationship between multiple independent variables and a single dependent variable using a linear equation. It extends simple linear regression by accommodating multiple predictors, offering a more comprehensive analysis of the factors influencing the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?**\n",
    "\n",
    "Multicollinearity occurs when predictors in a multiple linear regression model are highly correlated. To detect and address multicollinearity:\n",
    "\n",
    "- Calculate the Variance Inflation Factor (VIF) for each predictor. High VIF values indicate high multicollinearity.\n",
    "- Remove one of the highly correlated predictors or use techniques like principal component analysis (PCA) to reduce multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. Describe the polynomial regression model. How is it different from linear regression?**\n",
    "\n",
    "Polynomial regression is an extension of linear regression that models the relationship between predictors and the target as an nth-degree polynomial equation. It allows capturing non-linear relationships, fitting a curve rather than a straight line, unlike linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?**\n",
    "\n",
    "**Advantages:**\n",
    "- Captures non-linear relationships.\n",
    "- Provides a more flexible model.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Can lead to overfitting with a high polynomial degree.\n",
    "- Interpretability becomes challenging.\n",
    "\n",
    "Polynomial regression is preferred when evidence of non-linear relationships exists, but caution is needed to avoid overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
