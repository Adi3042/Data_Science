{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77d9872-4da5-420c-a507-f628a4275565",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\" style=\"padding: 10px;\">    \n",
    "    <h1><b><u>Ensemble Techniques And Its Types-1</u></b></h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c5c91-da1a-4ee3-8246-cebbe52c7e4c",
   "metadata": {},
   "source": [
    "**Q1. What is an ensemble technique in machine learning?**\n",
    "\n",
    "(i). An ensemble technique in machine learning is a method that combines the predictions of multiple base models to produce a single, more robust and accurate prediction.\n",
    "\n",
    "(ii). The idea behind ensemble techniques is that by aggregating the predictions from multiple models, the overall performance can be improved, often surpassing the performance of individual models.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. Why are ensemble techniques used in machine learning?**\n",
    "\n",
    "Ensemble techniques are used in machine learning for several reasons:-\n",
    "\n",
    "(i). They can improve predictive accuracy by reducing overfitting and bias.\n",
    "\n",
    "(ii). They are robust and can handle noisy or incomplete data.\n",
    "\n",
    "(iii). They can capture different aspects of the data by combining diverse models.\n",
    "\n",
    "(iv). They provide better generalization of the problem by reducing variance.\n",
    "\n",
    "(v). They are versatile and can be applied to various types of machine learning tasks, such as classification, regression, and more.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. What is bagging?**\n",
    "\n",
    "(i). Bagging, short for Bootstrap Aggregating, is an ensemble technique that involves training multiple base models (usually of the same type) on different bootstrap samples of the training data and then aggregating their predictions.\n",
    "\n",
    "(ii). Bagging aims to reduce variance and improve the stability of the model by averaging or taking a majority vote of the individual predictions.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. What is boosting?**\n",
    "\n",
    "(i). Boosting is another ensemble technique that aims to improve the performance of a machine learning model by combining the predictions of multiple weak base models (typically decision trees) sequentially.\n",
    "\n",
    "(ii). In boosting, each base model is trained to correct the errors made by the previous models, leading to a strong overall model.\n",
    "\n",
    "(iii). Common boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. What are the benefits of using ensemble techniques?**\n",
    "\n",
    "The benefits of using ensemble techniques in machine learning include:-\n",
    "\n",
    "(i). Improved predictive accuracy and generalization.\n",
    "\n",
    "(ii). Reduced overfitting and increased model robustness.\n",
    "\n",
    "(iii). Enhanced model stability and robustness to noisy data.\n",
    "\n",
    "(iv). The ability to capture complex relationships in the data.\n",
    "\n",
    "(v). Versatility, as ensemble methods can be applied to various machine learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. Are ensemble techniques always better than individual models?**\n",
    "\n",
    "(i). Ensemble techniques are not always better than individual models.\n",
    "\n",
    "(ii). Their effectiveness depends on the specific problem, the quality of the base models, and the diversity among them.\n",
    "\n",
    "(iii). In some cases, a well-tuned individual model may perform as well as or even better than an ensemble.\n",
    "\n",
    "(iv). Ensembles are particularly useful when dealing with complex, noisy, or high-dimensional data, where combining multiple models can improve overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. How is the confidence interval calculated using bootstrap?**\n",
    "\n",
    "The confidence interval using bootstrap is calculated by resampling the data with replacement to create a large number of bootstrap samples. For each bootstrap sample, you calculate the statistic of interest (e.g., mean, median) and create a distribution of these statistics.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. How does bootstrap work and What are the steps involved in bootstrap?**\n",
    "\n",
    "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic or parameter.\n",
    "\n",
    "Here are the steps involved in the bootstrap process:-\n",
    "\n",
    "(i). Randomly draw a sample (with replacement) of the same size as the original data from the dataset. This creates a bootstrap sample.\n",
    "\n",
    "(ii). Calculate the statistic of interest (e.g., mean, median) for the bootstrap sample.\n",
    "\n",
    "(iii). Repeat steps 1 and 2 a large number of times (typically thousands of times) to create a distribution of the statistic.\n",
    "\n",
    "(iv). Construct the confidence interval by finding the desired percentiles of the distribution. For example, for a 95% confidence interval, we can use the 2.5th and 97.5th percentiles.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b611f0c5-d260-4049-b772-ede86649a994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: (15.00, 15.00) meters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "sample_heights = np.array([15.0] * 50)  # Replace with actual data\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 10000\n",
    "\n",
    "bootstrap_means = np.zeros(num_samples)\n",
    "\n",
    "# Generate bootstrap samples and calculate means\n",
    "for i in range(num_samples):\n",
    "    bootstrap_sample = np.random.choice(sample_heights, size=50, replace=True)\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "lower_percentile = np.percentile(bootstrap_means, 2.5)\n",
    "upper_percentile = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"95% Confidence Interval: ({lower_percentile:.2f}, {upper_percentile:.2f}) meters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
