{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d30ff9-fa6c-4ba4-9497-c39f20eb082a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\" style=\"padding: 10px;\">    \n",
    "    <h1><b><u>Ensemble Techniques And Its Types-3</u></b></h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca9714-a03a-48a6-adce-c3e4231a7900",
   "metadata": {},
   "source": [
    "**Q1. What is Random Forest Regressor?**\n",
    "\n",
    "Random Forest Regressor is a machine learning algorithm used for regression tasks. It is an ensemble method that combines multiple decision trees to make predictions about continuous numerical values (i.e., regression targets). Each decision tree in the ensemble is trained on a random subset of the data and a random subset of features, adding an element of randomness and diversity to the model.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. How does Random Forest Regressor reduce the risk of overfitting?**\n",
    "\n",
    "Random Forest Regressor reduces the risk of overfitting through two main mechanisms:\n",
    "\n",
    "(i). **Bootstrap Sampling:**\n",
    "    - Each decision tree in the ensemble is trained on a random bootstrap sample of the data (sampling with replacement). This randomness helps prevent any single tree from memorizing the training data, reducing overfitting.\n",
    "\n",
    "(ii). **Feature Subsampling:**\n",
    "    - At each split in the tree, only a random subset of features is considered for the split. This further adds diversity and reduces the chance of individual trees overfitting to specific features.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?**\n",
    "\n",
    "Random Forest Regressor aggregates predictions by averaging the outputs of its constituent decision trees. For a regression task, the final prediction is often the mean (average) of the predictions made by all the individual decision trees. This ensemble averaging helps improve the overall prediction accuracy and stability.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. What are the hyperparameters of Random Forest Regressor?**\n",
    "\n",
    "Some of the hyperparameters of the Random Forest Regressor include:\n",
    "\n",
    "- `n_estimators`: The number of decision trees in the ensemble.\n",
    "- `max_depth`: The maximum depth of each decision tree.\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node.\n",
    "- `max_features`: The number of features to consider for each split.\n",
    "- `bootstrap`: Whether to use bootstrap sampling.\n",
    "- `random_state`: A seed for random number generation to ensure reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?**\n",
    "\n",
    "Difference between Random Forest Regressor and Decision Tree Regressor are as follows:\n",
    "\n",
    "(i). Random Forest Regressor is an ensemble method composed of multiple decision trees, while a Decision Tree Regressor is a single decision tree.\n",
    "\n",
    "(ii). Random Forest Regressor reduces overfitting by averaging the predictions of multiple trees, whereas a single Decision Tree Regressor is prone to overfitting.\n",
    "\n",
    "(iii). Random Forest Regressor is more robust and generalizes better due to its ensemble nature.\n",
    "\n",
    "(iv). Decision Tree Regressor is simple and interpretable but can be less accurate and more prone to overfitting on complex data.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. What are the advantages and disadvantages of Random Forest Regressor?**\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "(i). High accuracy and generalization.\n",
    "\n",
    "(ii). Robust to overfitting due to ensemble averaging.\n",
    "\n",
    "(iii). Handles large feature sets and high-dimensional data.\n",
    "\n",
    "(iv). Provides feature importances for feature selection.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "(i). Complexity and longer training time compared to a single decision tree.\n",
    "\n",
    "(ii). Lack of interpretability when dealing with many trees.\n",
    "\n",
    "(iii). May not perform well on small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. What is the output of Random Forest Regressor?**\n",
    "\n",
    "The output of a Random Forest Regressor is a continuous numerical value, which is the predicted regression target for a given input.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. Can Random Forest Regressor be used for classification tasks?**\n",
    "\n",
    "While Random Forest Regressor is primarily designed for regression tasks (predicting continuous numerical values), Random Forest can also be used for classification tasks by employing a similar ensemble approach. In classification, it's called a Random Forest Classifier. Instead of averaging predictions, it typically uses majority voting to classify data points into classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
