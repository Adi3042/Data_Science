{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ce8b83-af66-4266-9c16-095ee0117171",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\" style=\"padding: 10px;\">\n",
    "<h1><b><u>Boosting-2</u></b></h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afcae31-c7d0-4a3a-b72c-f4a41a4405c7",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a powerful machine learning algorithm used for both regression and classification tasks. In the context of regression, it builds a predictive model by combining the predictions of multiple weak learners (typically decision trees) sequentially. The key idea is to minimize the error of the model at each step by fitting a weak learner to the residuals of the previous model. This process continues until a predefined number of iterations are reached or until the model converges to a certain level of performance.\n",
    "\n",
    "**Here are some examples of problems that can be solved with Gradient Boosting Regression:**\n",
    "\n",
    "* Predicting house prices\n",
    "* Predicting customer churn\n",
    "* Predicting fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cddc28-6ddc-4b3e-8904-a9ad44a4d591",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1dbcc1-abd9-40a4-86a7-857e23d8fcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.82\n",
      "R-squared: 0.64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Initialize the target variable as residuals\n",
    "F = np.zeros_like(y)\n",
    "\n",
    "# Set the number of boosting iterations\n",
    "n_iterations = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Create weak learners\n",
    "weak_learners = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Calculate residuals\n",
    "    residuals = y - F\n",
    "    weak_learner = DecisionTreeRegressor(max_depth=1)\n",
    "    weak_learner.fit(X, residuals)\n",
    "    F += learning_rate * weak_learner.predict(X)\n",
    "    weak_learners.append(weak_learner)\n",
    "\n",
    "# Predict using the ensemble of weak learners\n",
    "y_pred = sum(learning_rate * weak_learner.predict(X) for weak_learner in weak_learners)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y, y_pred)\n",
    "print(f\"R-squared: {r_squared:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65aabc-6f94-4d32-acab-4d7e0e4deaf0",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0690be9a-371f-4057-9e8f-970dbadd05b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "Mean Squared Error: 0.85\n",
      "R-squared: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [1, 2]\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(gbm, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X)\n",
    "mse = ((y - y_pred) ** 2).mean()\n",
    "r_squared = best_model.score(X, y)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r_squared:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c8722-8c3f-4839-a466-69b8475cef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "\n",
    "    A weak learner in the context of Gradient Boosting is a base model that performs slightly better than random guessing but\n",
    "    is not necessarily a strong or highly accurate model on its own. \n",
    "    Weak learners are often simple models, such as decision stumps or linear models. \n",
    "    The key characteristic of a weak learner is that its error rate is only slightly better than chance, which allows Gradient \n",
    "    Boosting to focus on the mistakes made by these models and iteratively improve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c4e89-18c6-4ce4-8522-dd4b4fb0f55d",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A weak learner in Gradient Boosting is a base model that performs slightly better than random guessing but is not necessarily a strong or highly accurate model on its own. Weak learners are often simple models, such as decision stumps or linear models. The key characteristic of a weak learner is that its error rate is only slightly better than chance, which allows Gradient Boosting to focus on the mistakes made by these models and iteratively improve predictions.\n",
    "\n",
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by sequentially combining the predictions of multiple weak learners.\n",
    "\n",
    "Here's a high-level intuition:\n",
    "\n",
    "* Start with an initial prediction.\n",
    "* Fit a weak learner to the residuals.\n",
    "* Update the predictions by adding a fraction (learning rate) of the weak learners predictions to the current predictions.\n",
    "* Repeat steps 2 and 3 iteratively, each time focusing on the mistakes made by the previous models.\n",
    "* The final prediction is a combination of all weak learners' predictions.\n",
    "\n",
    "The algorithm effectively reduces the errors in predictions at each step and converges to a strong predictive model.\n",
    "\n",
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners by iteratively training and combining them. The process can be summarized as follows:\n",
    "\n",
    "* Initialize the ensemble with a constant prediction.\n",
    "* For each iteration:\n",
    "    * Calculate the negative gradient of the loss with respect to the current predictions. This represents the direction and magnitude of the error to be corrected.\n",
    "    * Fit a weak learner to the negative gradient to predict how the current models predictions should be adjusted.\n",
    "    * Update the ensembles predictions by adding a fraction of the predictions from the weak learner.\n",
    "    * Repeat steps a-c for a predefined number of iterations.\n",
    "\n",
    "The final prediction is the sum of predictions from all weak learners, which collectively forms a strong predictive model.\n",
    "\n",
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "Here are the key steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm:\n",
    "\n",
    "1. Define a loss function to measure prediction errors (e.g., mean squared error).\n",
    "2. Initialize predictions with a constant value (e.g., mean of target values).\n",
    "3. Calculate the negative gradient of the loss function with respect to current predictions.\n",
    "4. Train a weak learner (e.g., decision tree) to predict the negative gradient (correction).\n",
    "5. Update predictions by adding a fraction (learning rate) of the weak learner's predictions.\n",
    "6. Repeat steps 3-5 iteratively for a predefined number of boosting rounds.\n",
    "\n",
    "The final prediction is the sum of predictions from all weak learners, forming a strong predictive model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
